{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3e00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import csv\n",
    "import json\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import threading\n",
    "import math\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from object_tracking import *\n",
    "import time\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bcbd74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\bedo-/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-18 Python-3.9.16 torch-2.0.0 CUDA:0 (NVIDIA GeForce GTX 1660, 6144MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\bedo-\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)\n",
    "model.eval();\n",
    "\n",
    "class Image_Classifier(nn.Module):\n",
    "    def init(self):\n",
    "        super().init()\n",
    "        self.model = nn.Sequential(\n",
    "             Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "             ReLU(),\n",
    "             Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "             ReLU(),\n",
    "             MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "             Flatten(start_dim=1, end_dim=-1),\n",
    "             Dropout(p=0.25, inplace=False),\n",
    "             Linear(in_features=6272, out_features=132, bias=True),\n",
    "             ReLU(),\n",
    "             Dropout(p=0.5, inplace=False),\n",
    "             Linear(in_features=132, out_features=11, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "color_classifier = torch.load(\"2layers_colour_model.pt\")\n",
    "color_classifier = color_classifier.cuda()\n",
    "color_classifier.eval()\n",
    "\n",
    "body_classifier = torch.load('model.pt')\n",
    "body_classifier.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1710a306",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "Video processing completed\n",
      "25.560031175613403\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    video_path = 'light traffic.mp4'\n",
    "    output_path = 'track_test.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    ffmpeg_cmd = f\"ffmpeg -y -f rawvideo -pix_fmt bgr24 -s {frame_width}x{frame_height} -r {fps} -i - -c:v libx264 -preset fast -crf 30 -pix_fmt nv12 -an -vcodec libx264 {output_path}\"\n",
    "\n",
    "    output_file = subprocess.Popen(ffmpeg_cmd.split(' '), stdin=subprocess.PIPE)\n",
    "    \n",
    "    mot_tracker = Sort(max_age=30, min_hits=60) \n",
    "\n",
    "    object_dict = {}\n",
    "\n",
    "    frame_cut = 0\n",
    "    frame_count = 0\n",
    "    clf_state = False\n",
    "#     color_thread = threading.Thread(target=color_classifier)\n",
    "#     color_thread.start()\n",
    "\n",
    "#     body_thread = threading.Thread(target=body_classifier)\n",
    "#     body_thread.start()\n",
    "    \n",
    "    start = time.time()\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print('Video processing completed')\n",
    "            break\n",
    "\n",
    "        frame_model = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = model(frame_model[frame_cut:])\n",
    "\n",
    "        track_bbs_ids = mot_tracker.update(results.xyxy[0][:,:4].cpu())\n",
    "        \n",
    "        for x1, y1, x2, y2, obj_id in track_bbs_ids:\n",
    "            cx1 = int((x1 + x2) / 2)\n",
    "            cy1 = int((y1 + y2) / 2)\n",
    "            width = abs(x2 - x1)\n",
    "            height = abs(y2 - y1)\n",
    "            diagonal = math.sqrt(width**2 + height**2)\n",
    "            \n",
    "            \n",
    "            if diagonal > 100 :\n",
    "                try: #TODO optimize\n",
    "                    car_image = frame_model[int(y1):int(y2), int(x1):int(x2)]\n",
    "                    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                    transforms.Resize((32,32)),])\n",
    "                    car_image = transform(car_image)\n",
    "                    car_image = car_image.cuda()\n",
    "                    \n",
    "                    body_car_image = frame_model[int(y1):int(y2), int(x1):int(x2)]\n",
    "                    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                    transforms.Resize((256,256)),])\n",
    "                    body_car_image = transform(body_car_image)\n",
    "                    body_car_image = body_car_image.cuda()\n",
    "                except:\n",
    "                    print('no object detected')\n",
    "                    continue\n",
    "                with torch.no_grad():\n",
    "                    color_output = color_classifier(car_image.unsqueeze(0))\n",
    "                    color_prediction = torch.argmax(color_output).item()\n",
    "                    color_name = ['black','blue','brown','green','grey','orange','pink','purple','red','white','yellow']\n",
    "                    color_class_name = color_name[color_prediction]\n",
    "\n",
    "                    body_output = body_classifier(body_car_image.unsqueeze(0))\n",
    "                    body_prediction = torch.argmax(body_output).item()\n",
    "                    body_name = ['Heavy-Duty', 'Lorry', 'Luxury', 'Pickup', 'SUV', 'Sedan', 'Van']\n",
    "                    body_class_name = body_name[body_prediction]\n",
    "                    clf_state = True\n",
    "\n",
    "            \n",
    "                if obj_id not in object_dict:\n",
    "                    object_dict[obj_id] = {\n",
    "                        'bboxes': [(x1, y1, x2, y2)],\n",
    "                        'frames':[frame_count],\n",
    "                        'last_seen_frame': 0,\n",
    "                        'color_classifier_preds': [color_prediction],\n",
    "                        'body_classifier_preds': [body_prediction]\n",
    "                    }\n",
    "                else:\n",
    "                    object_dict[obj_id]['bboxes'].append(( x1, y1, x2, y2))\n",
    "                    object_dict[obj_id]['frames'].append((frame_count))\n",
    "                    object_dict[obj_id]['color_classifier_preds'].append(color_prediction)\n",
    "                    object_dict[obj_id]['body_classifier_preds'].append(body_prediction)\n",
    "\n",
    "                object_dict[obj_id]['last_seen_frame'] = frame_count\n",
    "\n",
    "                # Calculate the mode prediction of the classifier for the tracked object\n",
    "                color_mode_pred = statistics.mode(object_dict[obj_id]['color_classifier_preds'])\n",
    "                object_dict[obj_id]['color_mode_pred'] = str(color_name[color_mode_pred])\n",
    "\n",
    "                body_mode_pred = statistics.mode(object_dict[obj_id]['body_classifier_preds'])\n",
    "                object_dict[obj_id]['body_mode_pred'] = str(body_name[body_mode_pred])\n",
    "\n",
    "            cv2.putText(frame, str(obj_id), (cx1, cy1), 0, 0.5, (255, 255, 255), 2)\n",
    "            if clf_state == True:\n",
    "                cv2.putText(frame, color_name[color_mode_pred], (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (203, 192, 255), 2)\n",
    "                cv2.putText(frame,  body_name[body_mode_pred], (int(cx1), int(y2)), cv2.FONT_HERSHEY_SIMPLEX, 1, (203, 192, 255), 2)\n",
    "                clf_state = False\n",
    "\n",
    "        output_file.stdin.write(frame.tobytes())\n",
    "\n",
    "\n",
    "        frame_count += 1\n",
    "#     color_thread.join()\n",
    "#     body_thread.join()\n",
    "    cap.release()\n",
    "    output_file.stdin.close()\n",
    "    output_file.wait()\n",
    "    end = time.time()\n",
    "    \n",
    "    print(end - start)\n",
    "    \n",
    "    with open(\"object_tracks.json\", \"w\") as f:\n",
    "        json.dump(object_dict, f, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c254a5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1621.93909,  187.92599, 1765.44250,  283.95273],\n",
       "        [1789.03381,  175.26503, 1919.26355,  277.11584],\n",
       "        [ 830.76105,   86.43929,  874.58551,  127.79375],\n",
       "        [   2.78865,  888.41809,  360.98422, 1078.31165],\n",
       "        [1732.77502,   33.43250, 1788.35486,   68.20553],\n",
       "        [ 862.72717,   18.53886,  900.86108,   49.74484],\n",
       "        [ 995.87817,   73.63623, 1039.29956,  116.14877],\n",
       "        [ 991.27368,   46.02561, 1035.75000,   96.28868],\n",
       "        [1505.88318,    3.30035, 1546.27954,   25.46384],\n",
       "        [1619.51416,  180.94038, 1920.00000,  277.43561]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.xyxy[0][:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8b36d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791.0547222461435 175.44359519107914 1924.1391523925397 279.381310085834 50.0\n",
      "862.5076409704981 18.507258348607568 900.999890648785 50.0153013766313 46.0\n",
      "1623.5478982117977 188.45009006717754 1768.832003121114 285.6204869503526 43.0\n",
      "995.9485593610135 74.02485747631778 1039.6077471537883 116.63913170862818 41.0\n",
      "830.4881467479073 86.60317803000335 874.670299832159 128.308336826105 40.0\n",
      "-4.108739180834164 892.7713896274313 357.7180696017726 1086.2534768464468 23.0\n"
     ]
    }
   ],
   "source": [
    "for x1,y1,x2,y2, obj_id in track_bbs_ids:\n",
    "    print(x1,y1,x2,y2, obj_id)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64a7350f",
   "metadata": {},
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "transform = T.ToPILImage()\n",
    "img = transform(car_image)\n",
    "img.show()\n",
    "output=classifier(car_image.unsqueeze(0))\n",
    "prediction = torch.argmax(output).item()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3db507c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_frames_and_last_seen(object_id, object_dict):\n",
    "    obj_data = object_dict[object_id]\n",
    "    frames = obj_data['frames']\n",
    "    last_seen = obj_data['last_seen_frame']\n",
    "    return frames, last_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56634688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_objects_by_prediction(color_mode, body_mode, object_dict):\n",
    "    object_ids = []\n",
    "    for obj_id, obj_data in object_dict.items():\n",
    "        if obj_data['color_mode_pred'] == color_mode and obj_data['body_mode_pred'] == body_mode:\n",
    "            object_ids.append(obj_id)\n",
    "    return object_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "865392a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iter(v, w): v must be callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m frames , bboxes \u001b[38;5;241m=\u001b[39m object_dict[obj_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m'\u001b[39m], object_dict[obj_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m cap\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m1\u001b[39m,frames[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame, bbox, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mrange\u001b[39m(frames[\u001b[38;5;241m0\u001b[39m], frames[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) , \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[43m)\u001b[49m) :\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;241m==\u001b[39m frames[i]:\n\u001b[0;32m     15\u001b[0m         centers \u001b[38;5;241m=\u001b[39m (bbox[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m bbox[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m , (bbox[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m bbox[\u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: iter(v, w): v must be callable"
     ]
    }
   ],
   "source": [
    "color_mode = 'grey'\n",
    "body_mode = 'Sedan'\n",
    "data = open('object_tracks.json')\n",
    "object_dict = json.load(data)\n",
    "\n",
    "object_ids = find_objects_by_prediction(color_mode, body_mode, object_dict)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "for obj_id in object_ids:\n",
    "    frames , bboxes = object_dict[obj_id]['frames'], object_dict[obj_id]['bboxes']\n",
    "\n",
    "    cap.set(1,frames[0])\n",
    "\n",
    "    for frame in range(frames[-1] -  frames[0]):\n",
    "        if frame == frames[frame]:\n",
    "            \n",
    "            centers = (bbox[0] + bbox[1])/2 , (bbox[2] + bbox[3])/2\n",
    "            cv2.putText(frame, str(f\"{obj_id} {color_mode} {body_mode}\"), centers, 0, 0.5, (255, 255, 255), 2)\n",
    "        ret, frame = cap.read()\n",
    "            \n",
    "\n",
    "\n",
    "    print(f'Object {obj_id} appeared in frames {frames} and was last seen in frame {last_seen}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3627ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'light traffic.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(\"preview\")\n",
    "cv2.imshow(\"preview\", img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b957441",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cv2.imshow('s',frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b91ef449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'green'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_dict['1.0']['color_mode_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41c87ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'light traffic.mp4'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20aaed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d257727d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (4107792713.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    *centers\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "*centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38718829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3dec83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1037.9927871895225, 182.18867663140293, 1116.660612863935, 251.26304904431527]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8051e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1037.7649355288627, 177.73996894204728, 1114.5109504255483, 243.91061705529538]\n"
     ]
    }
   ],
   "source": [
    "for bbox in bboxes: print(bbox) ; break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fddbd9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(range(63, 107),\n",
       " [[1037.7649355288627,\n",
       "   177.73996894204728,\n",
       "   1114.5109504255483,\n",
       "   243.91061705529538],\n",
       "  [1037.830288889742, 179.173655772369, 1115.572178412243, 246.5299337903159],\n",
       "  [1037.9927871895225,\n",
       "   182.18867663140293,\n",
       "   1116.660612863935,\n",
       "   251.26304904431527],\n",
       "  [1037.4665031818388,\n",
       "   186.7161226316257,\n",
       "   1117.0226830639206,\n",
       "   257.92517825052437],\n",
       "  [1038.9018082293521,\n",
       "   191.77212298515974,\n",
       "   1119.9191858327993,\n",
       "   265.0137016153457],\n",
       "  [1039.6787690056751,\n",
       "   197.59479092845618,\n",
       "   1122.09812731935,\n",
       "   273.16298690749613],\n",
       "  [1041.4450898849964,\n",
       "   204.07106970091098,\n",
       "   1125.0363689786477,\n",
       "   281.2182704737604],\n",
       "  [1041.8767513259716,\n",
       "   206.11427637252774,\n",
       "   1126.289111178249,\n",
       "   284.3942955266585],\n",
       "  [1042.3223885314392,\n",
       "   210.5358015833063,\n",
       "   1128.6724276567052,\n",
       "   291.06437288250964],\n",
       "  [1042.9032243333086,\n",
       "   217.35532996385382,\n",
       "   1131.8371037861216,\n",
       "   299.5987176585187],\n",
       "  [1044.7087380252888,\n",
       "   224.29263699852714,\n",
       "   1136.463851390331,\n",
       "   308.8004103272471],\n",
       "  [1045.7209992777332,\n",
       "   232.06985622036296,\n",
       "   1140.6135681492958,\n",
       "   319.13586515387505],\n",
       "  [1046.4333944935352,\n",
       "   240.13071047329618,\n",
       "   1144.1731384178997,\n",
       "   329.209974277087],\n",
       "  [1046.1821454230299,\n",
       "   242.6598531914239,\n",
       "   1145.9582015447318,\n",
       "   333.1581179798961],\n",
       "  [1046.8570260587358,\n",
       "   249.60091046226353,\n",
       "   1148.8604244773019,\n",
       "   342.33976679655916],\n",
       "  [1048.2203535966823,\n",
       "   257.94573274170347,\n",
       "   1152.978726316951,\n",
       "   353.7574417923915],\n",
       "  [1049.191729483939,\n",
       "   268.1234925041086,\n",
       "   1157.3796818541482,\n",
       "   367.07156109486056],\n",
       "  [1050.6990274617222,\n",
       "   277.22100100213186,\n",
       "   1162.2602211778037,\n",
       "   380.48201122145383],\n",
       "  [1051.9511325999592,\n",
       "   288.6314165348379,\n",
       "   1167.468501502342,\n",
       "   395.87132327413104],\n",
       "  [1051.7030732470034,\n",
       "   292.0382271093394,\n",
       "   1169.9835264874232,\n",
       "   402.0802852897172],\n",
       "  [1053.2372384222815,\n",
       "   300.6098139624075,\n",
       "   1174.6500498649723,\n",
       "   415.75180734192355],\n",
       "  [1054.2687624495115, 312.9299377346179, 1179.496790147161, 432.518177123012],\n",
       "  [1057.3032222234162,\n",
       "   324.9859192187415,\n",
       "   1186.4542385155503,\n",
       "   449.4571241497217],\n",
       "  [1057.9931091931371,\n",
       "   340.10295384723275,\n",
       "   1191.9012646737128,\n",
       "   470.2035790525637],\n",
       "  [1059.931146144716,\n",
       "   355.78394886980186,\n",
       "   1199.0340581893051,\n",
       "   492.85742943214314],\n",
       "  [1059.8096460353734,\n",
       "   360.2368923184475,\n",
       "   1202.4709170710623,\n",
       "   502.2887443279795],\n",
       "  [1060.7403684957262,\n",
       "   374.1686050338008,\n",
       "   1208.0119192754828,\n",
       "   523.0976583744687],\n",
       "  [1062.4405048297845,\n",
       "   392.07261124244064,\n",
       "   1215.4109726637598,\n",
       "   547.9888390198316],\n",
       "  [1064.8464111614664,\n",
       "   411.83376950217144,\n",
       "   1224.1625575595833,\n",
       "   575.3903832146297],\n",
       "  [1067.2257467316308,\n",
       "   431.944640091245,\n",
       "   1232.9087104987534,\n",
       "   604.9770255521707],\n",
       "  [1070.8934981025927,\n",
       "   455.97201576939483,\n",
       "   1243.904124335481,\n",
       "   640.2777985019029],\n",
       "  [1071.1150378408472,\n",
       "   462.74737074072954,\n",
       "   1249.072788894019,\n",
       "   655.1025313782238],\n",
       "  [1074.2223121192644,\n",
       "   482.41421648303236,\n",
       "   1259.0172323521513,\n",
       "   686.1108347197342],\n",
       "  [1078.7844740006876,\n",
       "   509.8869590838085,\n",
       "   1270.958296847899,\n",
       "   727.6005161029816],\n",
       "  [1084.2136283731309,\n",
       "   540.7529771279872,\n",
       "   1285.3561873522983,\n",
       "   775.7580890182064],\n",
       "  [1089.1644543591835,\n",
       "   577.1294183961985,\n",
       "   1301.3111962236846,\n",
       "   831.8191799646344],\n",
       "  [1092.9382154277575,\n",
       "   616.6492162727858,\n",
       "   1318.7066945124973,\n",
       "   895.886085821567],\n",
       "  [1092.393392028237, 626.870451045655, 1326.9844864613501, 923.6756214781749],\n",
       "  [1094.0914545518237,\n",
       "   662.6579270175105,\n",
       "   1342.6459559184098,\n",
       "   984.8612164370372],\n",
       "  [1101.7988515106733,\n",
       "   702.0045925103673,\n",
       "   1364.3953335787435,\n",
       "   1043.6403844033841],\n",
       "  [1109.1803435256284,\n",
       "   736.7045749759691,\n",
       "   1386.5998789542896,\n",
       "   1076.1469423418837],\n",
       "  [1115.8095326613545,\n",
       "   782.1792054398443,\n",
       "   1424.5101019573751,\n",
       "   1097.114403750985],\n",
       "  [1111.6260122360845,\n",
       "   836.4860367433255,\n",
       "   1426.0982739100791,\n",
       "   1114.2675801890953],\n",
       "  [1109.9596381545039,\n",
       "   860.9554964133389,\n",
       "   1425.6897398309047,\n",
       "   1115.2843602518371],\n",
       "  [1104.0663646442501,\n",
       "   916.5159395852136,\n",
       "   1433.9161970431899,\n",
       "   1121.4325793751175]],\n",
       " range(0, 45))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(frames[0], frames[-1]) , bboxes , range(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0440be3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z = iter(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4bff801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in z : print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e3174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
