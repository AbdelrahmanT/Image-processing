{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3e00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import csv\n",
    "import json\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import threading\n",
    "import math\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from object_tracking import *\n",
    "import time\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bcbd74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\ahmed/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-18 Python-3.9.13 torch-2.0.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 6144MiB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\ahmed\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)\n",
    "model.eval();\n",
    "\n",
    "class Image_Classifier(nn.Module):\n",
    "    def init(self):\n",
    "        super().init()\n",
    "        self.model = nn.Sequential(\n",
    "             Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "             ReLU(),\n",
    "             Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "             ReLU(),\n",
    "             MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "             Flatten(start_dim=1, end_dim=-1),\n",
    "             Dropout(p=0.25, inplace=False),\n",
    "             Linear(in_features=6272, out_features=132, bias=True),\n",
    "             ReLU(),\n",
    "             Dropout(p=0.5, inplace=False),\n",
    "             Linear(in_features=132, out_features=11, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "color_classifier = torch.load(\"2layers_colour_model.pt\")\n",
    "color_classifier = color_classifier.cuda()\n",
    "color_classifier.eval()\n",
    "\n",
    "body_classifier = torch.load('model.pt')\n",
    "body_classifier.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ff44fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "no object detected\n",
      "27\n",
      "no object detected\n",
      "28\n",
      "no object detected\n",
      "29\n",
      "no object detected\n",
      "30\n",
      "no object detected\n",
      "31\n",
      "no object detected\n",
      "32\n",
      "no object detected\n",
      "33\n",
      "no object detected\n",
      "34\n",
      "no object detected\n",
      "35\n",
      "no object detected\n",
      "36\n",
      "no object detected\n",
      "37\n",
      "1\n",
      "no object detected\n",
      "Video processing completed\n",
      "18.609665155410767\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    video_path = 'light traffic.mp4'\n",
    "    output_path = 'track_test.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    ffmpeg_cmd = f\"ffmpeg -y -f rawvideo -pix_fmt bgr24 -s {frame_width}x{frame_height} -r {fps} -i - -c:v libx264 -preset fast -crf 30 -pix_fmt nv12 -an -vcodec libx264 {output_path}\"\n",
    "\n",
    "    output_file = subprocess.Popen(ffmpeg_cmd.split(' '), stdin=subprocess.PIPE)\n",
    "    \n",
    "    mot_tracker = Sort(max_age=30, min_hits=60) \n",
    "\n",
    "    object_dict = {}\n",
    "\n",
    "    frame_count = 0\n",
    "    clf_state = False \n",
    "\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((32,32)),]) \n",
    "    body_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((256,256)),])\n",
    "    \n",
    "    batch_images = []\n",
    "    batch_body_images = []\n",
    "    \n",
    "    color_name = ['black','blue','brown','green','grey','orange','pink','purple','red','white','yellow']\n",
    "    body_name = ['Heavy-Duty', 'Lorry', 'Luxury', 'Pickup', 'SUV', 'Sedan', 'Van']\n",
    "\n",
    "    start = time.time()     \n",
    "    while True:         \n",
    "        ret, frame = cap.read()        \n",
    "        if not ret:             \n",
    "            print('Video processing completed')            \n",
    "            break  \n",
    "\n",
    "        frame_model = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n",
    "        results = model(frame_model)\n",
    "\n",
    "        track_bbs_ids = mot_tracker.update(results.xyxy[0][:,:4].cpu())\n",
    "\n",
    "        for x1, y1, x2, y2, obj_id in track_bbs_ids:\n",
    "            cx1 = int((x1 + x2) / 2)\n",
    "            cy1 = int((y1 + y2) / 2)\n",
    "            width = abs(x2 - x1)\n",
    "            height = abs(y2 - y1)\n",
    "            diagonal = math.sqrt(width**2 + height**2)\n",
    "            if obj_id not in object_dict:\n",
    "                object_dict[obj_id] = {\n",
    "                    'bboxes': [(x1, y1, x2, y2)],\n",
    "                    'frames':[frame_count],\n",
    "                    'color_classifier_preds': [],\n",
    "                    'body_classifier_preds': [],\n",
    "                    'color_mode_pred':str(),\n",
    "                    'body_mode_pred':str()}\n",
    "\n",
    "            if diagonal > 100:\n",
    "                \n",
    "                try:\n",
    "                    car_image = frame_model[int(y1):int(y2), int(x1):int(x2)]\n",
    "                    car_image = transform(car_image)\n",
    "                    car_image = car_image.cuda()\n",
    "\n",
    "                    body_car_image = frame_model[int(y1):int(y2), int(x1):int(x2)]\n",
    "                    body_car_image = body_transform(body_car_image)\n",
    "                    body_car_image = body_car_image.cuda()\n",
    "\n",
    "                    batch_images.append(car_image)\n",
    "                    batch_body_images.append(body_car_image)\n",
    "                    print(len(batch_images))\n",
    "                except:\n",
    "                    print('no object detected')\n",
    "                    continue\n",
    "\n",
    "                if len(batch_images) >= 37:\n",
    "                    with torch.no_grad():\n",
    "                        color_output = color_classifier(torch.stack(batch_images).cuda())\n",
    "                        color_prediction = torch.argmax(color_output, dim=1)\n",
    "                    batch_images.clear()\n",
    "\n",
    "                if len(batch_body_images) >= 37:\n",
    "                    with torch.no_grad():\n",
    "                        body_output = body_classifier(torch.stack(batch_body_images).cuda())\n",
    "                        body_prediction = torch.argmax(body_output, dim=1)\n",
    "                        clf_state = True\n",
    "                    batch_body_images.clear()\n",
    "                    if obj_id not in object_dict:\n",
    "                        object_dict[obj_id] = {\n",
    "                            'bboxes': [(x1, y1, x2, y2)],\n",
    "                            'frames':[frame_count],\n",
    "                            'last_seen_frame': 0,\n",
    "                            'color_classifier_preds': [tuple(color_prediction.cpu().numpy())],\n",
    "                            'body_classifier_preds': [tuple(body_prediction.cpu().numpy())]\n",
    "                        }\n",
    "                    else:\n",
    "                        object_dict[obj_id]['bboxes'].append(( x1, y1, x2, y2))\n",
    "                        object_dict[obj_id]['frames'].append((frame_count))\n",
    "                        object_dict[obj_id]['color_classifier_preds'].append(tuple(color_prediction.cpu().numpy()))\n",
    "                        object_dict[obj_id]['body_classifier_preds'].append(tuple(body_prediction.cpu().numpy()))\n",
    "                    object_dict[obj_id]['last_seen_frame'] = frame_count\n",
    "\n",
    "                    color_mode_pred = statistics.mode(object_dict[obj_id]['color_classifier_preds'])\n",
    "                    color_mode_pred = int(color_mode_pred[0])\n",
    "                    object_dict[obj_id]['color_mode_pred'] = color_name[color_mode_pred]\n",
    "\n",
    "                    body_mode_pred = statistics.mode(object_dict[obj_id]['body_classifier_preds'])\n",
    "                    body_mode_pred = int(body_mode_pred[0])\n",
    "                    object_dict[obj_id]['body_mode_pred'] = body_name[body_mode_pred]\n",
    "                    with open(\"object_tracks.json\", \"w\") as f:        \n",
    "                        json.dump(object_dict, f, indent=4, default=lambda x: x.tolist())\n",
    "\n",
    "            cv2.putText(frame, str(obj_id), (cx1, cy1), 0, 0.5, (255, 255, 255), 2)\n",
    "            if clf_state == True:               \n",
    "                cv2.putText(frame, color_name[color_mode_pred], (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (203, 192, 255), 2)                 \n",
    "                cv2.putText(frame, body_name[body_mode_pred], (int(cx1), int(y2)), cv2.FONT_HERSHEY_SIMPLEX, 1, (203, 192, 255), 2)               \n",
    "                clf_state = False    \n",
    "                    \n",
    "        output_file.stdin.write(frame.tobytes())   \n",
    "        frame_count += 1\n",
    "    cap.release() \n",
    "    output_file.stdin.close()  \n",
    "    output_file.wait()\n",
    "    end = time.time()     \n",
    "    print(end - start)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c254a5fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1621.93909,  187.92599, 1765.44250,  283.95273],\n",
       "        [1789.03381,  175.26503, 1919.26355,  277.11584],\n",
       "        [ 830.76105,   86.43929,  874.58551,  127.79375],\n",
       "        [   2.78865,  888.41809,  360.98422, 1078.31165],\n",
       "        [1732.77502,   33.43250, 1788.35486,   68.20553],\n",
       "        [ 862.72717,   18.53886,  900.86108,   49.74484],\n",
       "        [ 995.87817,   73.63623, 1039.29956,  116.14877],\n",
       "        [ 991.27368,   46.02561, 1035.75000,   96.28868],\n",
       "        [1505.88318,    3.30035, 1546.27954,   25.46384],\n",
       "        [1619.51416,  180.94038, 1920.00000,  277.43561]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.xyxy[0][:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8b36d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791.0547222461435 175.44359519107914 1924.1391523925397 279.381310085834 50.0\n",
      "862.5076409704981 18.507258348607568 900.999890648785 50.0153013766313 46.0\n",
      "1623.5478982117977 188.45009006717754 1768.832003121114 285.6204869503526 43.0\n",
      "995.9485593610135 74.02485747631778 1039.6077471537883 116.63913170862818 41.0\n",
      "830.4881467479073 86.60317803000335 874.670299832159 128.308336826105 40.0\n",
      "-4.108739180834164 892.7713896274313 357.7180696017726 1086.2534768464468 23.0\n"
     ]
    }
   ],
   "source": [
    "for x1,y1,x2,y2, obj_id in track_bbs_ids:\n",
    "    print(x1,y1,x2,y2, obj_id)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64a7350f",
   "metadata": {},
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "transform = T.ToPILImage()\n",
    "img = transform(car_image)\n",
    "img.show()\n",
    "output=classifier(car_image.unsqueeze(0))\n",
    "prediction = torch.argmax(output).item()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3db507c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_frames_and_last_seen(object_id, object_dict):\n",
    "    obj_data = object_dict[object_id]\n",
    "    frames = obj_data['frames']\n",
    "    last_seen = obj_data['last_seen_frame']\n",
    "    return frames, last_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56634688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_objects_by_prediction(color_mode, body_mode, object_dict):\n",
    "    object_ids = []\n",
    "    for obj_id, obj_data in object_dict.items():\n",
    "        if obj_data['color_mode_pred'] == color_mode and obj_data['body_mode_pred'] == body_mode:\n",
    "            object_ids.append(obj_id)\n",
    "    return object_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "865392a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_search(json_file, input_video, output_video='test.mp4',car_colour ='', car_body = ''):\n",
    "    \"\"\"\n",
    "    Search for car's body and colour in a given video using the video's json file\n",
    "    json_file : a file which contains all information about cars in the given videos\n",
    "                where the value of the json is the car's ID\n",
    "    \"\"\"\n",
    "\n",
    "    data = open('object_tracks.json')\n",
    "    object_dict = json.load(data)\n",
    "\n",
    "    object_ids = find_objects_by_prediction(car_colour, car_body, object_dict)\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    ffmpeg_cmd = f\"ffmpeg -y -f rawvideo -pix_fmt bgr24 -s {frame_width}x{frame_height} -r {fps} -i - -c:v libx264 -preset fast -crf 30 -pix_fmt nv12 -an -vcodec libx264 {output_video}\"\n",
    "    output_file = subprocess.Popen(ffmpeg_cmd.split(' '), stdin=subprocess.PIPE)\n",
    "\n",
    "\n",
    "    for obj_id in object_ids:\n",
    "        frames , bboxes = object_dict[obj_id]['frames'], object_dict[obj_id]['bboxes']\n",
    "\n",
    "        cap.set(1,frames[1])\n",
    "        j = 0\n",
    "        for i in range(frames[1],frames[-1]):\n",
    "            bbox = bboxes[j]\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print('Error, frame doesn\\'t exist')\n",
    "                break\n",
    "\n",
    "            if i == frames[j]:\n",
    "                x,y = int((bbox[0] + bbox[2])/2 ), int( (bbox[1] + bbox[3])/2)\n",
    "                cv2.putText(frame,  f'{obj_id}, {car_colour}, {car_body}', (int(x), int(y)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "                print(x,y)\n",
    "            output_file.stdin.write(frame.tobytes())\n",
    "            j+=1\n",
    "        print(f'Object {obj_id} appeared in frames {frames}.')\n",
    "\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    output_file.stdin.close()\n",
    "    output_file.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5db99d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 144.0 appeared in frames [0, 44].\n",
      "Object 186.0 appeared in frames [294, 310].\n",
      "Object 193.0 appeared in frames [334, 334].\n"
     ]
    }
   ],
   "source": [
    "car_search('tracked_objects.json','light traffic.mp4','test.mp4','grey','Sedan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3627ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'light traffic.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(\"preview\")\n",
    "cv2.imshow(\"preview\", img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b957441",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cv2.imshow('s',frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b91ef449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'green'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_dict['1.0']['color_mode_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41c87ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'light traffic.mp4'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20aaed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23007042",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (4107792713.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    *centers\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "*centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6251ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
