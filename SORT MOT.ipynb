{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3e00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import csv\n",
    "import json\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import threading\n",
    "import math\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from object_tracking import *\n",
    "import time\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bcbd74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\bedo-/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-18 Python-3.9.16 torch-2.0.0 CUDA:0 (NVIDIA GeForce GTX 1660, 6144MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\bedo-\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)\n",
    "model.eval();\n",
    "\n",
    "class Image_Classifier(nn.Module):\n",
    "    def init(self):\n",
    "        super().init()\n",
    "        self.model = nn.Sequential(\n",
    "             Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "             ReLU(),\n",
    "             Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "             ReLU(),\n",
    "             MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "             Flatten(start_dim=1, end_dim=-1),\n",
    "             Dropout(p=0.25, inplace=False),\n",
    "             Linear(in_features=6272, out_features=132, bias=True),\n",
    "             ReLU(),\n",
    "             Dropout(p=0.5, inplace=False),\n",
    "             Linear(in_features=132, out_features=11, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "color_classifier = torch.load(\"2layers_colour_model.pt\")\n",
    "color_classifier = color_classifier.cuda()\n",
    "color_classifier.eval()\n",
    "\n",
    "body_classifier = torch.load('model.pt')\n",
    "body_classifier.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1710a306",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "Video processing completed\n",
      "26.762486934661865\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    video_path = 'light traffic.mp4'\n",
    "    output_path = 'track_test.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    ffmpeg_cmd = f\"ffmpeg -y -f rawvideo -pix_fmt bgr24 -s {frame_width}x{frame_height} -r {fps} -i - -c:v libx264 -preset fast -crf 30 -pix_fmt nv12 -an -vcodec libx264 {output_path}\"\n",
    "\n",
    "    output_file = subprocess.Popen(ffmpeg_cmd.split(' '), stdin=subprocess.PIPE)\n",
    "    \n",
    "    mot_tracker = Sort(max_age=30, min_hits=60) \n",
    "\n",
    "    object_dict = {}\n",
    "\n",
    "    frame_cut = 0\n",
    "    frame_count = 0\n",
    "    clf_state = False\n",
    "#     color_thread = threading.Thread(target=color_classifier)\n",
    "#     color_thread.start()\n",
    "\n",
    "#     body_thread = threading.Thread(target=body_classifier)\n",
    "#     body_thread.start()\n",
    "    \n",
    "    start = time.time()\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print('Video processing completed')\n",
    "            break\n",
    "\n",
    "        frame_model = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = model(frame_model[frame_cut:])\n",
    "\n",
    "        track_bbs_ids = mot_tracker.update(results.xyxy[0][:,:4].cpu())\n",
    "        \n",
    "        for x1, y1, x2, y2, obj_id in track_bbs_ids:\n",
    "            cx1 = int((x1 + x2) / 2)\n",
    "            cy1 = int((y1 + y2) / 2)\n",
    "            width = abs(x2 - x1)\n",
    "            height = abs(y2 - y1)\n",
    "            diagonal = math.sqrt(width**2 + height**2)\n",
    "            \n",
    "            \n",
    "            if diagonal > 100 :\n",
    "                try: #TODO optimize\n",
    "                    car_image = frame_model[int(y1):int(y2), int(x1):int(x2)]\n",
    "                    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                    transforms.Resize((32,32)),])\n",
    "                    car_image = transform(car_image)\n",
    "                    car_image = car_image.cuda()\n",
    "                    \n",
    "                    body_car_image = frame_model[int(y1):int(y2), int(x1):int(x2)]\n",
    "                    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                    transforms.Resize((256,256)),])\n",
    "                    body_car_image = transform(body_car_image)\n",
    "                    body_car_image = body_car_image.cuda()\n",
    "                except:\n",
    "                    print('no object detected')\n",
    "                    continue\n",
    "                with torch.no_grad():\n",
    "                    color_output = color_classifier(car_image.unsqueeze(0))\n",
    "                    color_prediction = torch.argmax(color_output).item()\n",
    "                    color_name = ['black','blue','brown','green','grey','orange','pink','purple','red','white','yellow']\n",
    "                    color_class_name = color_name[color_prediction]\n",
    "\n",
    "                    body_output = body_classifier(body_car_image.unsqueeze(0))\n",
    "                    body_prediction = torch.argmax(body_output).item()\n",
    "                    body_name = ['Heavy-Duty', 'Lorry', 'Luxury', 'Pickup', 'SUV', 'Sedan', 'Van']\n",
    "                    body_class_name = body_name[body_prediction]\n",
    "                    clf_state = True\n",
    "\n",
    "            \n",
    "                if obj_id not in object_dict:\n",
    "                    object_dict[obj_id] = {\n",
    "                        'bboxes': [(x1, y1, x2, y2)],\n",
    "                        'frames':[frame_count],\n",
    "                        'last_seen_frame': 0,\n",
    "                        'color_classifier_preds': [color_prediction],\n",
    "                        'body_classifier_preds': [body_prediction]\n",
    "                    }\n",
    "                else:\n",
    "                    object_dict[obj_id]['bboxes'].append(( x1, y1, x2, y2))\n",
    "                    object_dict[obj_id]['frames'].append((frame_count))\n",
    "                    object_dict[obj_id]['color_classifier_preds'].append(color_prediction)\n",
    "                    object_dict[obj_id]['body_classifier_preds'].append(body_prediction)\n",
    "\n",
    "                object_dict[obj_id]['last_seen_frame'] = frame_count\n",
    "\n",
    "                # Calculate the mode prediction of the classifier for the tracked object\n",
    "                color_mode_pred = statistics.mode(object_dict[obj_id]['color_classifier_preds'])\n",
    "                object_dict[obj_id]['color_mode_pred'] = str(color_name[color_mode_pred])\n",
    "\n",
    "                body_mode_pred = statistics.mode(object_dict[obj_id]['body_classifier_preds'])\n",
    "                object_dict[obj_id]['body_mode_pred'] = str(body_name[body_mode_pred])\n",
    "\n",
    "            cv2.putText(frame, str(obj_id), (cx1, cy1), 0, 0.5, (255, 255, 255), 2)\n",
    "            if clf_state == True:\n",
    "                cv2.putText(frame, color_name[color_mode_pred], (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (203, 192, 255), 2)\n",
    "                cv2.putText(frame,  body_name[body_mode_pred], (int(cx1), int(y2)), cv2.FONT_HERSHEY_SIMPLEX, 1, (203, 192, 255), 2)\n",
    "                clf_state = False\n",
    "\n",
    "        output_file.stdin.write(frame.tobytes())\n",
    "\n",
    "\n",
    "        frame_count += 1\n",
    "#     color_thread.join()\n",
    "#     body_thread.join()\n",
    "    cap.release()\n",
    "    output_file.stdin.close()\n",
    "    output_file.wait()\n",
    "    end = time.time()\n",
    "    \n",
    "    print(end - start)\n",
    "    \n",
    "    with open(\"object_tracks.json\", \"w\") as f:\n",
    "        json.dump(object_dict, f, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c254a5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1621.93909,  187.92599, 1765.44250,  283.95273],\n",
       "        [1789.03381,  175.26503, 1919.26355,  277.11584],\n",
       "        [ 830.76105,   86.43929,  874.58551,  127.79375],\n",
       "        [   2.78865,  888.41809,  360.98422, 1078.31165],\n",
       "        [1732.77502,   33.43250, 1788.35486,   68.20553],\n",
       "        [ 862.72717,   18.53886,  900.86108,   49.74484],\n",
       "        [ 995.87817,   73.63623, 1039.29956,  116.14877],\n",
       "        [ 991.27368,   46.02561, 1035.75000,   96.28868],\n",
       "        [1505.88318,    3.30035, 1546.27954,   25.46384],\n",
       "        [1619.51416,  180.94038, 1920.00000,  277.43561]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.xyxy[0][:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8b36d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791.0547222461435 175.44359519107914 1924.1391523925397 279.381310085834 50.0\n",
      "862.5076409704981 18.507258348607568 900.999890648785 50.0153013766313 46.0\n",
      "1623.5478982117977 188.45009006717754 1768.832003121114 285.6204869503526 43.0\n",
      "995.9485593610135 74.02485747631778 1039.6077471537883 116.63913170862818 41.0\n",
      "830.4881467479073 86.60317803000335 874.670299832159 128.308336826105 40.0\n",
      "-4.108739180834164 892.7713896274313 357.7180696017726 1086.2534768464468 23.0\n"
     ]
    }
   ],
   "source": [
    "for x1,y1,x2,y2, obj_id in track_bbs_ids:\n",
    "    print(x1,y1,x2,y2, obj_id)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64a7350f",
   "metadata": {},
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "transform = T.ToPILImage()\n",
    "img = transform(car_image)\n",
    "img.show()\n",
    "output=classifier(car_image.unsqueeze(0))\n",
    "prediction = torch.argmax(output).item()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3db507c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_frames_and_last_seen(object_id, object_dict):\n",
    "    obj_data = object_dict[object_id]\n",
    "    frames = obj_data['frames']\n",
    "    last_seen = obj_data['last_seen_frame']\n",
    "    return frames, last_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56634688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_objects_by_prediction(color_mode, body_mode, object_dict):\n",
    "    object_ids = []\n",
    "    for obj_id, obj_data in object_dict.items():\n",
    "        if obj_data['color_mode_pred'] == color_mode and obj_data['body_mode_pred'] == body_mode:\n",
    "            object_ids.append(obj_id)\n",
    "    return object_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "865392a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 4.0 appeared in frames [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107] and was last seen in frame 107.\n",
      "Object 26.0 appeared in frames [216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252] and was last seen in frame 252.\n"
     ]
    }
   ],
   "source": [
    "color_mode = 'grey'\n",
    "body_mode = 'Sedan'\n",
    "data = open('object_tracks.json')\n",
    "object_dict = json.load(data)\n",
    "\n",
    "object_ids = find_objects_by_prediction(color_mode, body_mode, object_dict)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "for obj_id in object_ids:\n",
    "    frames,bboxes = object_dict['frames'], object_dict['bboxes']\n",
    "    centers = (bboxes[0] + bboxes[1])/2 + (bboxes[2] + bboxes[3])/2  \n",
    "    cap.set(1,frames[0])\n",
    "    ret, frame = cap.read()\n",
    "    for frame, i in range(frames[0], frames[-1]), range(len(frames)):\n",
    "        if frame == frames[i]:\n",
    "            cv2.putText(frame, str(f\"{obj_id} {color_mode} {body_mode}\"), (cx1, cy1), 0, 0.5, (255, 255, 255), 2)\n",
    "            \n",
    "    cv2.imshow('z',frame)\n",
    "    cv2.waitKey(0)\n",
    "    print(f'Object {obj_id} appeared in frames {frames} and was last seen in frame {last_seen}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3627ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'light traffic.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(\"preview\")\n",
    "cv2.imshow(\"preview\", img)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b957441",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cv2.imshow('s',frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b91ef449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'green'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_dict['1.0']['color_mode_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "41c87ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'light traffic.mp4'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20aaed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23007042",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (4107792713.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    *centers\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": [
    "*centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6251ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
