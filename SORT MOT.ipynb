{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3e00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import csv\n",
    "import json\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import threading\n",
    "# from object_detection import ObjectDetection\n",
    "import math\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from object_tracking import *\n",
    "# import maths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bcbd74c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\bedo-/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-4-18 Python-3.9.16 torch-2.0.0 CUDA:0 (NVIDIA GeForce GTX 1660, 6144MiB)\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m C:\\Users\\bedo-\\.cache\\torch\\hub\\requirements.txt not found, check failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5m', pretrained=True)\n",
    "model.eval();\n",
    "\n",
    "class Image_Classifier(nn.Module):\n",
    "    def init(self):\n",
    "        super().init()\n",
    "        self.model = nn.Sequential(\n",
    "             Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "             ReLU(),\n",
    "             Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
    "             ReLU(),\n",
    "             MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "             Flatten(start_dim=1, end_dim=-1),\n",
    "             Dropout(p=0.25, inplace=False),\n",
    "             Linear(in_features=6272, out_features=132, bias=True),\n",
    "             ReLU(),\n",
    "             Dropout(p=0.5, inplace=False),\n",
    "             Linear(in_features=132, out_features=11, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "color_classifier = torch.load(\"2layers_colour_model.pt\")\n",
    "color_classifier = color_classifier.cuda()\n",
    "color_classifier.eval()\n",
    "\n",
    "body_classifier = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1710a306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bedo-\\anaconda3\\envs\\pytorch2.0\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\bedo-\\anaconda3\\envs\\pytorch2.0\\lib\\threading.py\", line 917, in run\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bedo-\\anaconda3\\envs\\pytorch2.0\\lib\\threading.py\", line 980, in _bootstrap_inner\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\bedo-\\anaconda3\\envs\\pytorch2.0\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    self.run()\n",
      "  File \"C:\\Users\\bedo-\\anaconda3\\envs\\pytorch2.0\\lib\\threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\bedo-\\anaconda3\\envs\\pytorch2.0\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: forward() missing 1 required positional argument: 'x'\n",
      "    return forward_call(*args, **kwargs)\n",
      "TypeError: forward() missing 1 required positional argument: 'x'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "no object detected\n",
      "Video processing completed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    video_path = 'light traffic.mp4'\n",
    "    output_path = 'track_test.mp4'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    ffmpeg_cmd = f\"ffmpeg -y -f rawvideo -pix_fmt bgr24 -s {frame_width}x{frame_height} -r {fps} -i - -c:v libx264 -preset fast -crf 30 -pix_fmt nv12 -an -vcodec libx264 {output_path}\"\n",
    "\n",
    "    output_file = subprocess.Popen(ffmpeg_cmd.split(' '), stdin=subprocess.PIPE)\n",
    "    \n",
    "    mot_tracker = Sort(max_age=30, min_hits=60) \n",
    "\n",
    "    object_dict = {}\n",
    "\n",
    "    frame_cut = 0\n",
    "    frame_count = 0\n",
    "    clf_state = False\n",
    "    color_thread = threading.Thread(target=color_classifier)\n",
    "    color_thread.start()\n",
    "\n",
    "    body_thread = threading.Thread(target=body_classifier)\n",
    "    body_thread.start()\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print('Video processing completed')\n",
    "            break\n",
    "\n",
    "        frame_model = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        results = model(frame_model[frame_cut:])\n",
    "\n",
    "        track_bbs_ids = mot_tracker.update(results.xyxy[0][:,:4].cpu())\n",
    "        \n",
    "        \n",
    "        for x1, y1, x2, y2, obj_id in track_bbs_ids:\n",
    "            cx1 = int((x1 + x2) / 2)\n",
    "            cy1 = int((y1 + y2) / 2)\n",
    "            width = abs(x2 - x1)\n",
    "            height = abs(y2 - y1)\n",
    "            diagonal = math.sqrt(width**2 + height**2)\n",
    "            \n",
    "            if diagonal > 60 :\n",
    "                try:\n",
    "                    car_image = frame_model[int(y1):int(y2), int(x1):int(x2)]\n",
    "                    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                    transforms.Resize((32,32)),])\n",
    "                    car_image = transform(car_image)\n",
    "                    car_image = car_image.cuda()\n",
    "                except:\n",
    "                    print('no object detected')\n",
    "                    continue\n",
    "                with torch.no_grad():\n",
    "                    color_output = color_classifier(car_image.unsqueeze(0))\n",
    "                    color_prediction = torch.argmax(color_output).item()\n",
    "                    color_name = ['black','blue','brown','green','grey','orange','pink','purple','red','white','yellow']\n",
    "                    color_class_name = color_name[color_prediction]\n",
    "\n",
    "                try:\n",
    "                    body_car_image = frame_model[int(y1):int(y2), int(x1):int(x2)]\n",
    "                    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                    transforms.Resize((256,256)),])\n",
    "                    body_car_image = transform(body_car_image)\n",
    "                    body_car_image = body_car_image.cuda()\n",
    "                except:\n",
    "                    print('no object detected')\n",
    "                    continue\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    body_output = body_classifier(body_car_image.unsqueeze(0))\n",
    "                    body_prediction = torch.argmax(body_output).item()\n",
    "                    body_name = ['Heavy-Duty', 'Lorry', 'Luxury', 'Pickup', 'SUV', 'Sedan', 'Van']\n",
    "                    body_class_name = body_name[body_prediction]\n",
    "                    clf_state = True\n",
    "\n",
    "            \n",
    "                if obj_id not in object_dict:\n",
    "                    object_dict[obj_id] = {\n",
    "                        'bboxes': [(frame_count, x1, y1, x2, y2)],\n",
    "                        'last_seen_frame': 0,\n",
    "                        'color_classifier_preds': [color_prediction],\n",
    "                        'body_classifier_preds': [body_prediction]\n",
    "                    }\n",
    "                else:\n",
    "                    object_dict[obj_id]['bboxes'].append((frame_count, x1, y1, x2, y2))\n",
    "                    object_dict[obj_id]['color_classifier_preds'].append(color_prediction)\n",
    "                    object_dict[obj_id]['body_classifier_preds'].append(body_prediction)\n",
    "\n",
    "                object_dict[obj_id]['last_seen_frame'] = frame_count\n",
    "\n",
    "                # Calculate the mode prediction of the classifier for the tracked object\n",
    "                color_mode_pred = statistics.mode(object_dict[obj_id]['color_classifier_preds'])\n",
    "                object_dict[obj_id]['color_mode_pred'] = color_mode_pred\n",
    "\n",
    "                body_mode_pred = statistics.mode(object_dict[obj_id]['body_classifier_preds'])\n",
    "                object_dict[obj_id]['body_mode_pred'] = body_mode_pred\n",
    "\n",
    "            cv2.putText(frame, str(obj_id), (cx1, cy1), 0, 0.5, (255, 255, 255), 2)\n",
    "            if clf_state == True:\n",
    "                cv2.putText(frame, color_name[color_mode_pred], (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 1, (203, 192, 255), 2)\n",
    "                cv2.putText(frame,  body_name[body_mode_pred], (int(cx1), int(y2)), cv2.FONT_HERSHEY_SIMPLEX, 1, (203, 192, 255), 2)\n",
    "\n",
    "        output_file.stdin.write(frame.tobytes())\n",
    "\n",
    "        for obj_id, obj_data in list(object_dict.items()):\n",
    "            if frame_count - obj_data['last_seen_frame'] > mot_tracker.max_age:\n",
    "                object_dict.pop(obj_id)\n",
    "\n",
    "        frame_count += 1\n",
    "    color_thread.join()\n",
    "    body_thread.join()\n",
    "    cap.release()\n",
    "    output_file.stdin.close()\n",
    "    output_file.wait()\n",
    "    \n",
    "    with open(\"object_tracks.json\", \"w\") as f:\n",
    "        json.dump(object_dict, f, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.xyxy[0][:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b36d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x1,y1,x2,y2, obj_id in track_bbs_ids:\n",
    "    print(x1,y1,x2,y2, obj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "transform = T.ToPILImage()\n",
    "img = transform(car_image)\n",
    "img.show()\n",
    "output=classifier(car_image.unsqueeze(0))\n",
    "prediction = torch.argmax(output).item()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee34f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db507c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
